{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation & Threshold Selection\n",
    "\n",
    "## Objective\n",
    "The objective of this notebook is to evaluate model performance using business-relevant metrics and select an operating threshold that balances default risk and loan approval volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df = pd.read_csv(\"../data/processed/model_data.csv\")\n",
    "\n",
    "X = df.drop(columns=[\"default\"])\n",
    "y = df[\"default\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_test_proba = model.predict_proba(X_test_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7002167729099027)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_test_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[179845,  89343],\n",
       "       [ 25543,  41597]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_05 = (y_test_proba >= 0.5).astype(int)\n",
    "\n",
    "confusion_matrix(y_test, y_pred_05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.67      0.76    269188\n",
      "           1       0.32      0.62      0.42     67140\n",
      "\n",
      "    accuracy                           0.66    336328\n",
      "   macro avg       0.60      0.64      0.59    336328\n",
      "weighted avg       0.76      0.66      0.69    336328\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 55374, 213814],\n",
       "       [  3489,  63651]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_03 = (y_test_proba >= 0.3).astype(int)\n",
    "\n",
    "confusion_matrix(y_test, y_pred_03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.21      0.34    269188\n",
      "           1       0.23      0.95      0.37     67140\n",
      "\n",
      "    accuracy                           0.35    336328\n",
      "   macro avg       0.59      0.58      0.35    336328\n",
      "weighted avg       0.80      0.35      0.34    336328\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_03))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_threshold(threshold):\n",
    "    y_pred = (y_test_proba >= threshold).astype(int)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    return {\n",
    "        \"threshold\": threshold,\n",
    "        \"recall_default\": report[\"1\"][\"recall\"],\n",
    "        \"precision_default\": report[\"1\"][\"precision\"],\n",
    "        \"false_negatives\": cm[1, 0],\n",
    "        \"false_positives\": cm[0, 1]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>recall_default</th>\n",
       "      <th>precision_default</th>\n",
       "      <th>false_negatives</th>\n",
       "      <th>false_positives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.993059</td>\n",
       "      <td>0.207145</td>\n",
       "      <td>466</td>\n",
       "      <td>255197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.948034</td>\n",
       "      <td>0.229402</td>\n",
       "      <td>3489</td>\n",
       "      <td>213814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.826393</td>\n",
       "      <td>0.266554</td>\n",
       "      <td>11656</td>\n",
       "      <td>152669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.619556</td>\n",
       "      <td>0.317680</td>\n",
       "      <td>25543</td>\n",
       "      <td>89343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  recall_default  precision_default  false_negatives  \\\n",
       "0        0.2        0.993059           0.207145              466   \n",
       "1        0.3        0.948034           0.229402             3489   \n",
       "2        0.4        0.826393           0.266554            11656   \n",
       "3        0.5        0.619556           0.317680            25543   \n",
       "\n",
       "   false_positives  \n",
       "0           255197  \n",
       "1           213814  \n",
       "2           152669  \n",
       "3            89343  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds = [0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "results = pd.DataFrame([evaluate_threshold(t) for t in thresholds])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresh = precision_recall_curve(y_test, y_test_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "threshold                 0.300000\n",
       "recall_default            0.948034\n",
       "precision_default         0.229402\n",
       "false_negatives        3489.000000\n",
       "false_positives      213814.000000\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# business constraint\n",
    "MIN_RECALL = 0.94\n",
    "\n",
    "# filter thresholds that meet recall requirement\n",
    "eligible_thresholds = results[results[\"recall_default\"] >= MIN_RECALL]\n",
    "\n",
    "# choose threshold with highest precision among eligible ones\n",
    "best_threshold = eligible_thresholds.sort_values(\n",
    "    by=\"precision_default\", ascending=False\n",
    ").iloc[0]\n",
    "\n",
    "best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FINAL_THRESHOLD = best_threshold[\"threshold\"]\n",
    "FINAL_THRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.21      0.34    269188\n",
      "           1       0.23      0.95      0.37     67140\n",
      "\n",
      "    accuracy                           0.35    336328\n",
      "   macro avg       0.59      0.58      0.35    336328\n",
      "weighted avg       0.80      0.35      0.34    336328\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_final_pred = (y_test_proba >= FINAL_THRESHOLD).astype(int)\n",
    "\n",
    "confusion_matrix(y_test, y_final_pred)\n",
    "print(classification_report(y_test, y_final_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Selection Rationale\n",
    "\n",
    "Given the asymmetric cost of errors in credit risk, the operating threshold was chosen to prioritize recall for default cases, reducing false negatives at the expense of higher false positives. This aligns with a conservative lending strategy focused on capital preservation.\n",
    "\n",
    "### Programmatic Threshold Selection\n",
    "\n",
    "Rather than selecting a threshold heuristically, a business constraint was imposed requiring a minimum recall of 94% for default cases. Among thresholds satisfying this constraint, the threshold maximizing precision was selected. This approach reflects real-world banking practices where risk appetite is defined first, followed by optimization within acceptable risk limits.\n",
    "\n",
    "### Final Model Performance at Selected Threshold\n",
    "\n",
    "At the selected threshold of 0.3, the model achieves approximately 95% recall for default cases, significantly reducing the likelihood of approving high-risk borrowers. Although overall accuracy is lower due to an increase in false positives, this trade-off is intentional and aligns with banking practices where the cost of default outweighs the cost of rejecting creditworthy applicants.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
